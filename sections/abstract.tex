\begin{abstract}

\noindent The development of a sign language recognition model with high performance would be a great improvement in the communication with and within  deaf communities. The main issue preventing effective models lays on the low availability of labeled data which complicates the use of modern deep learning models of a big labeled data source. For this reason we compare a series of models and training procedures specially tailored for small datasets to improve the performance over previous state-of-the-art models. We trained DenseNet and few-shot Prototypical Network models with and without transfer learning, and also using Model-Agnostic Meta-Learning (MAML), a method specifically tailored to tackle few-shot learning. Our main findings indicate that DenseNet without transfer learning and Prototipical Networks with transfer learning provide the best results on most tasks. Using a meta-learning technique such as MAML does not seem to improve the performance of DenseNet w.r.t to traditional training. We also measure the effect of the training set size on the performance of the models, and found that, in general, prototypical networks are superior when using less than 30 samples.

\end{abstract}

\Keywords{sign language, handshape recognition, convolutional neural networks, densenet, prototypical networks, model-agnostic meta-learning, transfer learning, small datasets}
