\noindent Sign language is a system of communication that uses signs with the hands and other movements, including facial expressions and postures with the body to communicate meaning, commonly used by deaf people.

Sign Language Recognition is a field in the intersection of computer vision and language translation that seeks to create systems capable of translating videos of people speaking in sign language into text.

Sign language recognition presents an interesting challenge as the available data is limited compared to other problems for example speech recognition \cite{bragg2019}. The nature of the problem complicates the creation of new sign language datasets. Furthermore, merging datasets from different regions or countries is impossible since each sign language has its own syntax and semantic. Producing a model capable of recognizing sign language with high precision would improve the quality of life of many people since an automatic interpreter would facilitate communication between signers and non signers.

In this paper we address the low availability of data by implementing a variety of state-of-the-art convolutional neural network models and training techniques designed to tackle small labelled datasets. We compare two model architectures: Prototypical Network \cite{protonet} and DenseNet \cite{densenet}. We train these models with regular training and with Transfer Learning and Model Agnostic Meta Learning (MAML) \cite{DBLP:journals/corr/FinnAL17}.

DenseNet is a well known state-of-the-art model that has shown good performance for image classification even in cases where there is a low amount of labeled data\cite{densenet}. When the amount of available data is reduced even further, few-shot learning techniques are required. We chose Prototypical network as our specialized few-shot learning model. Prototypical networks are models based on metric learning, optimizing a distance function between classes in an embedded space to classify each sample.

In this work we propose to evaluate and compare new methods devoted to deal with small datasets in order to improve the current state-of-the-art in hand shape recognition for sign language.

Our approach consists of comparing different techniques for improving model performance in these conditions: prototypical networks for few shot learning,  transfer learning and model-agnostic meta-learning. We use the same data augmentation scheme for every experiment which consists of basic augmentation operators such as rotations, translations  and crops.

In the following subsection we summarize previous efforts on training CNN on handshape datasets. Section \ref{sec:datasets} describes the datasets and section \ref{sec:models} describes models and techniques we employed in our experiments, which are detailed along with results in Section \ref{sec:experiments}, and Section \ref{sec:conclusion} contains the conclusion of our work.

\subsection{Related Work}

Recent years have seen the rise in the use of deep learning models for sign language recognition, specifically the use of convolutional neural networks to extract image features or classify hand images. \cite{koller16}  trained a CNN to recognize handshapes  from the  RWTH handshape dataset,  which contains 3200 labeled samples and 50 different classes.  The model was based on a pre-trained network with a VGG architecture, and employed a semi-supervised scheme to take advantage of approximately  one million weakly labeled images, achieving an accuracy of 85.50\%. This constitutes the first attempt at adapting a model to overcome the low availability of labeled images for training. \cite{Ronchetti2016}  employed a radon transform as a feature for an ad hoc classifier that employed clustering as a quantization step and K nearest  neighbors  for the final classification. They tested the model on the LSA16 dataset, which contains only 800 examples, obtaining an accuracy of 92.3\%. \cite{quiroga2017study}  evaluated several CNNs on the LSA16 and RWTH datasets, including both vanilla and pre-trained models. The use of pretrained models helps to alleviate the lack of labeled data, since pretraining the convolutional filters establishes a prior that a classifier can exploit for handshape recognition. Their best models of an accuracy of 95.92\% for LSA16 and 82.88\%  for RWTH. \cite{ciarp2018} trained a simple neural network to classify a new dataset they created which we will call CIARP. CIARP contains 6000 examples and 10 classes, and the authores report an accuracy of 99.20\% with that model. \cite{tang2015real} train a CNN on a custom dataset with 36 classes, 8 subjects and 57000 sample images, obtaining an accuracy of 94.17\%. However, the samples correspond to video sequences and therefore are highly correlated;  while there are approximately 2000 images per class, there are only eight image sequences, one for each subject. Since each image sequence contains approximately 250 images  which are highly correlated, they only consider eight image per sequence per class. \cite{ameen2017fingerspelling} trained a simple CNN with only 6 layers using the ASL Finger spelling dataset, obtaining an accuracy of 80.34\%. The dataset consists of 60000 images of 25 different classes, but they were captured as videos so they are also highly correlated as in the previous case. \cite{barros14multichannel} employed the Jochen Triesch Database (JTD), which contains only 10 classes and  72 samples per class, as well as the NAO Camera Hand Posture Database, which contains 4 classes and 400 examples per class. They trained a simple CNN with a multichannel image containing the results of the Sobel operator as input, obtaining an F-score of 94\%  and  98\% in  each dataset perspective. \cite{alani2018peru} trained a deep CNN on the Hand Gesture Dataset LPD,  which contains 3250 images of only 6 classes, obtaining an accuracy of 99.73\%. \cite{cornejo2019hand} performed experiments with Wide-DenseNet and Prototypical Networks on the CIARP, LSA16 and RWTH datasets using vanilla models. In both cases, they also quantify the impact of data augmentation on accuracy. Their best models obtain an accuracy of 99.26\% on LSA16, 94.00\% on RWTH and 100.00\% on CIARP. This work is the third \cite{quiroga2017study,koller16:deephand} and last instance we found where a specific strategy was employed to alleviate the lack of data

This brief review confirms our previous statement that  while CNN are being consistently applied to handshape recognition tasks, most of these datasets are small and ad hoc, that is, recorded specifically for the purpose of testing a single model and not developed with the intent of providing a benchmark and complete training set for handshape recognition models. It is also worth noticing that some datasets are so small that it is very easy to obtain near-perfect performance with simple models. Also, many datasets are not readily available,  given that the authors have not published the data and do not provide any means of obtaining it.  

We note that the RWTH and LSA16 are both publicly available and current models have been shown to achieve less than perfect accuracy for them.  While the dataset in \cite{ciarp2018} (denoted CIARP in this paper) has been solved completely,  it is interesting and complementary because it targets general handshapes instead of those specific to sign language.
