@Inbook{Refaeilzadeh2009,
author="Refaeilzadeh, Payam
and Tang, Lei
and Liu, Huan",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="Cross-Validation",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="532--538",
isbn="978-0-387-39940-9",
doi="10.1007/978-0-387-39940-9_565",
url="https://doi.org/10.1007/978-0-387-39940-9_565"
}

@article{protonet,
  author    = {Jake Snell and
               Kevin Swersky and
               Richard S. Zemel},
  title     = {Prototypical Networks for Few-shot Learning},
  journal   = {CoRR},
  volume    = {abs/1703.05175},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.05175},
  archivePrefix = {arXiv},
  eprint    = {1703.05175},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SnellSZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VinyalsBLKW16,
  author    = {Oriol Vinyals and
               Charles Blundell and
               Timothy P. Lillicrap and
               Koray Kavukcuoglu and
               Daan Wierstra},
  title     = {Matching Networks for One Shot Learning},
  journal   = {CoRR},
  volume    = {abs/1606.04080},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.04080},
  archivePrefix = {arXiv},
  eprint    = {1606.04080},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/VinyalsBLKW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/FinnAL17,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  journal   = {CoRR},
  volume    = {abs/1703.03400},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.03400},
  archivePrefix = {arXiv},
  eprint    = {1703.03400},
  timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FinnAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q },
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}


@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778}
}

@article{Hu2017SqueezeandExcitationN,
  title={Squeeze-and-Excitation Networks},
  author={Jie Hu and Li Shen and Gang Sun},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={7132-7141}
}

@misc{Adam,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization
of stochastic objective functions, based on adaptive estimates of lower-order
moments. The method is straightforward to implement, is computationally
efficient, has little memory requirements, is invariant to diagonal rescaling
of the gradients, and is well suited for problems that are large in terms of
data and/or parameters. The method is also appropriate for non-stationary
objectives and problems with very noisy and/or sparse gradients. The
hyper-parameters have intuitive interpretations and typically require little
tuning. Some connections to related algorithms, on which Adam was inspired, are
discussed. We also analyze the theoretical convergence properties of the
algorithm and provide a regret bound on the convergence rate that is comparable
to the best known results under the online convex optimization framework.
Empirical results demonstrate that Adam works well in practice and compares
favorably to other stochastic optimization methods. Finally, we discuss AdaMax,
a variant of Adam based on the infinity norm.},
  added-at = {2019-06-04T16:24:16.000+0200},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/2d53bcfff0fe1a1d3a4a171352ee6e92c/alrigazzi},
  description = {Adam: A Method for Stochastic Optimization},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {d53bcfff0fe1a1d3a4a171352ee6e92c},
  keywords = {deep dl large-scale networks neural},
  note = {cite arxiv:1412.6980Comment: Published as a conference paper at the 3rd International Conference  for Learning Representations, San Diego, 2015},
  timestamp = {2019-06-04T16:24:16.000+0200},
  title = {Adam: A Method for Stochastic Optimization},
  url = {http://arxiv.org/abs/1412.6980},
  year = 2014
}

@InProceedings{pmlr-v80-pham18a,
  title = 	 {Efficient Neural Architecture Search via Parameters Sharing},
  author = 	 {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4095--4104},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pham18a/pham18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/pham18a.html},
  abstract = 	 {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89\% test error, which is on par with the 2.65\% test error of NASNet (Zoph et al., 2018).}
}


@book{Goodfellow2016DL,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 title = {Deep Learning},
 year = {2016},
 isbn = {0262035618, 9780262035613},
 publisher = {The MIT Press},
}

@inproceedings{cubuk2019autoaugment,
  title={AutoAugment: Learning Augmentation Strategies From Data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={113--123},
  year={2019}
}

@inproceedings{farhadi2007,
  author = {Farhadi, Ali and Forsyth, David and White, Ryan},
  year = {2007},
  month = {06},
  pages = {},
  title = {Transfer Learning in Sign language},
  journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  doi = {10.1109/CVPR.2007.383346}
}

@INPROCEEDINGS{allard2017,  
  author={U. {Côté-Allard} and C. L. {Fall} and A. {Campeau-Lecours} and C. {Gosselin} and F. {Laviolette} and B. {Gosselin}},  
  booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},   
  title={Transfer learning for sEMG hand gestures recognition using convolutional neural networks},   
  year={2017},  
  volume={},  
  number={},  
  pages={1663-1668}
}

@InProceedings{tan2018,
  author="Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang",
  editor="K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias",
  title="A Survey on Deep Transfer Learning",
  booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2018",
  year="2018",
  publisher="Springer International Publishing",
  address="Cham",
  pages="270--279",
  abstract="As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.",
  isbn="978-3-030-01424-7"
}

@article{weiss2016,
  author = {Weiss, Karl and Khoshgoftaar, Taghi and Wang, DingDing},
  year = {2016},
  month = {12},
  pages = {},
  title = {A survey of transfer learning},
  volume = {3},
  journal = {Journal of Big Data},
  doi = {10.1186/s40537-016-0043-6}
}

@article{cifar10dataset,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{bragg2019,
    author = {Bragg, Danielle and Koller, Oscar and Bellard, Mary and Berke, Larwan and Boudrealt, Patrick and Braffort, Annelies and Caselli, Naomi and Huenerfauth, Matt and Kacorri, Hernisa and Verhoef, Tessa and Vogler, Christian and Morris, Meredith},
    year = {2019},
    month = {08},
    pages = {},
    title = {Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective}
}